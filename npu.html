
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Appendix C: The concept of NPU (Neural Processor Unit) compiler &#8212; Tutorial: Creating an LLVM Backend for the Cpu0 Architecture</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/haiku.css" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Todo List" href="todo.html" />
    <link rel="prev" title="Appendix B: Cpu0 document and test" href="doc.html" /> 
  </head><body>
      <div class="header" role="banner"><h1 class="heading"><a href="index.html">
          <span>Tutorial: Creating an LLVM Backend for the Cpu0 Architecture</span></a></h1>
        <h2 class="heading"><span>Appendix C: The concept of NPU (Neural Processor Unit) compiler</span></h2>
      </div>
      <div class="topnav" role="navigation" aria-label="top navigation">
      
        <p>
        «&#160;&#160;<a href="doc.html">Appendix B: Cpu0 document and test</a>
        &#160;&#160;::&#160;&#160;
        <a class="uplink" href="index.html">Contents</a>
        &#160;&#160;::&#160;&#160;
        <a href="todo.html">Todo List</a>&#160;&#160;»
        </p>

      </div>
      <div class="content" role="main">
        
        
  <section id="appendix-c-the-concept-of-npu-neural-processor-unit-compiler">
<span id="sec-npu"></span><h1>Appendix C: The concept of NPU (Neural Processor Unit) compiler<a class="headerlink" href="#appendix-c-the-concept-of-npu-neural-processor-unit-compiler" title="Permalink to this heading">¶</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#deep-learning-theory" id="id39">Deep Learning Theory</a></p>
<ul>
<li><p><a class="reference internal" href="#deep-learning" id="id40">Deep Learning</a></p></li>
<li><p><a class="reference internal" href="#cnn" id="id41">CNN</a></p></li>
<li><p><a class="reference internal" href="#book" id="id42">Book</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#deep-learning-compiler" id="id43">Deep learning compiler</a></p>
<ul>
<li><p><a class="reference internal" href="#survey" id="id44">Survey</a></p></li>
<li><p><a class="reference internal" href="#gpu" id="id45">GPU</a></p></li>
<li><p><a class="reference internal" href="#nnef" id="id46">NNEF</a></p></li>
<li><p><a class="reference internal" href="#openvx" id="id47">OpenVX</a></p>
<ul>
<li><p><a class="reference internal" href="#sycl" id="id48">SYCL</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#tools" id="id49">Tools</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#npu-compiler" id="id50">NPU compiler</a></p>
<ul>
<li><p><a class="reference internal" href="#abstract" id="id51">Abstract</a></p></li>
<li><p><a class="reference internal" href="#mlir-and-iree" id="id52">MLIR and IREE</a></p></li>
<li><p><a class="reference internal" href="#tensorflow" id="id53">Tensorflow</a></p></li>
<li><p><a class="reference internal" href="#mlir-to-onnx" id="id54">mlir to onnx</a></p></li>
<li><p><a class="reference internal" href="#support-tensorflow" id="id55">Support tensorflow</a></p></li>
<li><p><a class="reference internal" href="#onnc" id="id56">ONNC</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#llvm-based-npu-compiler" id="id57">LLVM based NPU compiler</a></p>
<ul>
<li><p><a class="reference internal" href="#llvm-ir-for-npu-compiler" id="id58">llvm IR for NPU compiler</a></p></li>
<li><p><a class="reference internal" href="#open-source-project" id="id59">Open source project</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#onnx" id="id60">ONNX</a></p>
<ul>
<li><p><a class="reference internal" href="#viewer" id="id61">viewer</a></p></li>
<li><p><a class="reference internal" href="#install" id="id62">install</a></p></li>
<li><p><a class="reference internal" href="#onnx-api" id="id63">onnx api</a></p></li>
</ul>
</li>
</ul>
</nav>
<section id="deep-learning-theory">
<h2><a class="toc-backref" href="#id39" role="doc-backlink">Deep Learning Theory</a><a class="headerlink" href="#deep-learning-theory" title="Permalink to this heading">¶</a></h2>
<section id="deep-learning">
<h3><a class="toc-backref" href="#id40" role="doc-backlink">Deep Learning</a><a class="headerlink" href="#deep-learning" title="Permalink to this heading">¶</a></h3>
<p>Hung-Yi Lee’s video <a class="footnote-reference brackets" href="#hungyilee" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>.</p>
</section>
<section id="cnn">
<h3><a class="toc-backref" href="#id41" role="doc-backlink">CNN</a><a class="headerlink" href="#cnn" title="Permalink to this heading">¶</a></h3>
<p>CNN: They have applications in image and video recognition, recommender systems,
image classification, medical image analysis, natural language processing, and
financial time series <a class="footnote-reference brackets" href="#cnnwiki" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>.</p>
<p>Concept about how to apply Convolution and MaxPool to getting features from image <a class="footnote-reference brackets" href="#selectedpattern" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>.
Conv+MaxPool -&gt; get features map and downsize image, more Conv+MaxPool can filter image and higher
level of features and downsize more image. CNN model used in image recognition.</p>
<p>Concept and data applying in Deap Learning for different models of CNN <a class="footnote-reference brackets" href="#onnxmodelzoo" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>.</p>
</section>
<section id="book">
<h3><a class="toc-backref" href="#id42" role="doc-backlink">Book</a><a class="headerlink" href="#book" title="Permalink to this heading">¶</a></h3>
<p><a class="reference external" href="http://www.deeplearningbook.org/">http://www.deeplearningbook.org/</a> NTU: Hung-yi Lee MLDS (Machine Learing Deep +
Structure approaches), the trend for future implementation.</p>
</section>
</section>
<section id="deep-learning-compiler">
<h2><a class="toc-backref" href="#id43" role="doc-backlink">Deep learning compiler</a><a class="headerlink" href="#deep-learning-compiler" title="Permalink to this heading">¶</a></h2>
<section id="survey">
<h3><a class="toc-backref" href="#id44" role="doc-backlink">Survey</a><a class="headerlink" href="#survey" title="Permalink to this heading">¶</a></h3>
<p>The current open source compilers for deep learning <a class="footnote-reference brackets" href="#dlcs" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a> as follows,</p>
<figure class="align-center" id="dlcs-f">
<a class="reference internal image-reference" href="_images/dlcompilersurvey.png"><img alt="_images/dlcompilersurvey.png" src="_images/dlcompilersurvey.png" style="width: 769.0px; height: 754.0px;" /></a>
</figure>
<p>TVM compiler open source infrastructure supports most of DL Frameworks such as
TensorFlow, PyTorch…, and generating CUDA/OpenCL/OpenGL for gpu and LLVM for
cpu.</p>
</section>
<section id="gpu">
<h3><a class="toc-backref" href="#id45" role="doc-backlink">GPU</a><a class="headerlink" href="#gpu" title="Permalink to this heading">¶</a></h3>
<p>The NVIDIA CUDA Toolkit provides a development environment for creating
high-performance GPU-accelerated applications.
GPU-accelerated CUDA libraries enable acceleration across multiple domains such
as linear algebra, image and video processing, deep learning and graph
analytics <a class="footnote-reference brackets" href="#gpu4dl" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a>.</p>
<p>OpenCL runs on AMD GPUs and provides partial support for TensorFlow and PyTorch.
If you want to develop new networks some details might be missing, which could
prevent you from implementing the features you need <a class="footnote-reference brackets" href="#gpu4dl" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a>.
For instance, if ARM GPU doesn’t implement operation “Cosh” on TVM while a DL model
created from PyTorch generate “Cosh” operation, then it will fail to run
the DL model though TVM compile PyTorch model into OpenCL.
Once “Cosh” is implemented with kernel fuction “Cosh” in OpenCL by calling
GPU’s instructions, it can be fixed.</p>
</section>
<section id="nnef">
<h3><a class="toc-backref" href="#id46" role="doc-backlink">NNEF</a><a class="headerlink" href="#nnef" title="Permalink to this heading">¶</a></h3>
<p>Neural Network Exchange Format.</p>
<p>NPU or GPU can apply NNEF to supprt all AI models.</p>
<p>The goal of NNEF is to enable data scientists and engineers to easily transfer
trained networks from their chosen training framework into a wide variety of
inference engines. A stable, flexible and extensible standard that equipment
manufacturers can rely on is critical for the widespread deployment of neural
networks onto edge devices, and so NNEF encapsulates a complete description of
the structure, operations and parameters of a trained neural network,
independent of the training tools used to produce it and the inference engine
used to execute it <a class="footnote-reference brackets" href="#id33" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a>.</p>
<p>ONNX and NNEF are Complementary
ONNX moves quickly to track authoring framework updates
NNEF provides a stable bridge from training into edge inferencing engines
<a class="footnote-reference brackets" href="#id33" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a>.</p>
<figure class="align-center" id="nnefeco-f">
<a class="reference internal image-reference" href="_images/nnefeco.png"><img alt="_images/nnefeco.png" src="_images/nnefeco.png" style="width: 832.0px; height: 653.0px;" /></a>
</figure>
<p>ONNX import/export to NNEF, should edge NPU use NNEF <a class="footnote-reference brackets" href="#id33" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a>?</p>
</section>
<section id="openvx">
<h3><a class="toc-backref" href="#id47" role="doc-backlink">OpenVX</a><a class="headerlink" href="#openvx" title="Permalink to this heading">¶</a></h3>
<p>OpenVX enables the graph to be extended to include hardware architectures
that don’t support programmable APIs <a class="footnote-reference brackets" href="#id34" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a>.</p>
<p>It is designed by the Khronos Group to facilitate portable, optimized and
power-efficient processing of methods for vision algorithms <a class="footnote-reference brackets" href="#openvx-wiki" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a>.
Nothing about NPU I think.</p>
<section id="sycl">
<h4><a class="toc-backref" href="#id48" role="doc-backlink">SYCL</a><a class="headerlink" href="#sycl" title="Permalink to this heading">¶</a></h4>
<p>User use SYCL or DSL compile domain language into SYCL and run on OpenCL
hardwares <a class="footnote-reference brackets" href="#id35" id="id13" role="doc-noteref"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></a>. An example here <a class="footnote-reference brackets" href="#sycl-wiki" id="id14" role="doc-noteref"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></a>.</p>
<figure class="align-center" id="syclcompiler-f">
<a class="reference internal image-reference" href="_images/syclcompiler.png"><img alt="_images/syclcompiler.png" src="_images/syclcompiler.png" style="width: 865.0px; height: 420.0px;" /></a>
</figure>
<figure class="align-center" id="syclimp-f">
<a class="reference internal image-reference" href="_images/syclimp.png"><img alt="_images/syclimp.png" src="_images/syclimp.png" style="width: 868.0px; height: 558.0px;" /></a>
</figure>
<p><a class="reference external" href="https://github.com/Jonathan2251/nc/OpenCL_SYCL">https://github.com/Jonathan2251/nc/OpenCL_SYCL</a></p>
</section>
</section>
<section id="tools">
<h3><a class="toc-backref" href="#id49" role="doc-backlink">Tools</a><a class="headerlink" href="#tools" title="Permalink to this heading">¶</a></h3>
<p>Create onnx test file <a class="footnote-reference brackets" href="#onnx-editor" id="id15" role="doc-noteref"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></a>.</p>
</section>
</section>
<section id="npu-compiler">
<h2><a class="toc-backref" href="#id50" role="doc-backlink">NPU compiler</a><a class="headerlink" href="#npu-compiler" title="Permalink to this heading">¶</a></h2>
<section id="abstract">
<h3><a class="toc-backref" href="#id51" role="doc-backlink">Abstract</a><a class="headerlink" href="#abstract" title="Permalink to this heading">¶</a></h3>
<p>Tensorflow support unknown shape <a class="footnote-reference brackets" href="#tfunknownshape" id="id16" role="doc-noteref"><span class="fn-bracket">[</span>16<span class="fn-bracket">]</span></a>.
Though our npu support kernel call where kernel call is a set of
commands to npu to deal shape at run time, it is unefficiency.
As I remember mlit supports binding shape for unknown at compile-time
but not always work.
Luckily, we can customilze by redefining model to binding shape staticlly [20200412]</p>
</section>
<section id="mlir-and-iree">
<h3><a class="toc-backref" href="#id52" role="doc-backlink">MLIR and IREE</a><a class="headerlink" href="#mlir-and-iree" title="Permalink to this heading">¶</a></h3>
<p>IREE (Intermediate Representation Execution Environment, pronounced as “eerie”)
is an MLIR-based end-to-end compiler that lowers ML models to a unified IR
optimized for real-time mobile/edge inference against heterogeneous hardware
accelerators. IREE also provides flexible deployment solutions for the compiled
ML models <a class="footnote-reference brackets" href="#iree" id="id17" role="doc-noteref"><span class="fn-bracket">[</span>15<span class="fn-bracket">]</span></a> as the following figure.</p>
<figure class="align-center" id="iree-f">
<a class="reference internal image-reference" href="_images/IREE-Architecture.png"><img alt="_images/IREE-Architecture.png" src="_images/IREE-Architecture.png" style="width: 1609.0px; height: 877.0px;" /></a>
</figure>
<ul class="simple">
<li><p>HAL IR: Vulkan-like allocation and execution model encoding -&gt; on-line first-time compilation and save in cache. Executable compilation via architecture specific backend compiler plugins.</p></li>
<li><p>VM IR: Dynamic module linkage definitions (imports, exports, globals, etc) <a class="footnote-reference brackets" href="#vm-ir-dml" id="id18" role="doc-noteref"><span class="fn-bracket">[</span>17<span class="fn-bracket">]</span></a>.</p></li>
</ul>
<p>The purpose of mlir is:</p>
<ul class="simple">
<li><p>Connect cpu with mlir-to-llvm-ir.</p></li>
</ul>
</section>
<section id="tensorflow">
<h3><a class="toc-backref" href="#id53" role="doc-backlink">Tensorflow</a><a class="headerlink" href="#tensorflow" title="Permalink to this heading">¶</a></h3>
<p>The mechansim of Mlir and iree applied on tensorflow as the figure above section
is not fitted for off-line edge npu that stand alone without server-connection
for tunning weight of face detection’s purpose.
It is designed for on-line server-connected npu.
The gpu of supporting spirv is best candidate until this date 2020/5/12.</p>
<p>At beginning, tensorflow rely on api without fixed format such as ONNX <a class="footnote-reference brackets" href="#onnx-fmt" id="id19" role="doc-noteref"><span class="fn-bracket">[</span>19<span class="fn-bracket">]</span></a>.
As a result ONNX emerged and adopted for most of npu in their private backend
compiler. Google does not like to hire onnx as the format for npu backend compiler
onnx-mlir project <a class="footnote-reference brackets" href="#onnx-mlir" id="id20" role="doc-noteref"><span class="fn-bracket">[</span>20<span class="fn-bracket">]</span></a> which convert onnx to mlir dialect is sponsored
by Google I guess <a class="footnote-reference brackets" href="#onnx-mlir-sponsor" id="id21" role="doc-noteref"><span class="fn-bracket">[</span>21<span class="fn-bracket">]</span></a> for encourging new npu compiler
development hiring mlir as their compiler input (convert onnx to mlir then
handling mlir input).</p>
<p>With mlir and iree appear on tensorflow as a series of fixed formats in
tensorflow as section above. The hardware vendors for cloud server AI machine
with heterogeneous hardware accelerators will run tensorflow system
by supporting mlir/iree input format in their compilers more and more.
So, it is unavoidable that tensorflow system’s npu vendors have to support
mlir/iree input format beyond onnx. Or open source software or vendor software
appear to do transfer from mlir/iree to onnx. (python in tensorflow api allow
unknown type and shape size, so it cannot transer python api to onnx fully).</p>
<p>If lucky, google may hire onnx. Because onnx format is older than mlir
in history. In addition in aspect of format, mlir has mult-level mult-dialect and
more complicate while onnx is easy and better to understand (P.S. I don’t dig
into mlir yet).
Many AI models has supported onnx file format. For some AI model’s formats that
run on tensorflow without supporting onnx, aplly tensorflow-onnx open
source project <a class="footnote-reference brackets" href="#tf-onnx" id="id22" role="doc-noteref"><span class="fn-bracket">[</span>22<span class="fn-bracket">]</span></a> can convert tensorflow to onnx partly.</p>
<p>Onnx alliance may release some programs for transfering mlir to onnx for fighting
agiant mlir-iree growing in npu compiler but not at this moment.</p>
<p>For off-line edge npu that stand alone without server-connection
for tunning weight of face detection’s purpose, supprting mlir-iree compiler
may not necessary.</p>
</section>
<section id="mlir-to-onnx">
<h3><a class="toc-backref" href="#id54" role="doc-backlink">mlir to onnx</a><a class="headerlink" href="#mlir-to-onnx" title="Permalink to this heading">¶</a></h3>
<p><a class="reference external" href="https://www.tensorflow.org/mlir">https://www.tensorflow.org/mlir</a></p>
<p><a class="reference external" href="https://mlir.llvm.org/talks/">https://mlir.llvm.org/talks/</a></p>
<p><a class="reference external" href="https://llvm.org/devmtg/2019-04/talks.html#Tutorial_1">https://llvm.org/devmtg/2019-04/talks.html#Tutorial_1</a></p>
<ul class="simple">
<li><p>3 ppt in llvm tutorials</p></li>
</ul>
<p><a class="reference external" href="https://llvm.org/devmtg/2019-04/slides/Tutorial-AminiVasilacheZinenko-MLIR.pdf">https://llvm.org/devmtg/2019-04/slides/Tutorial-AminiVasilacheZinenko-MLIR.pdf</a></p>
<p>build mlir: <a class="reference external" href="https://mlir.llvm.org/getting_started/">https://mlir.llvm.org/getting_started/</a></p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">~/llvm/1/llvm-project/build$ cmake -G Ninja ../llvm \</span>
<span class="go">&gt;    -DLLVM_ENABLE_PROJECTS=mlir \</span>
<span class="go">&gt;    -DLLVM_BUILD_EXAMPLES=ON \</span>
<span class="go">&gt;    -DLLVM_TARGETS_TO_BUILD=&quot;X86;NVPTX;AMDGPU&quot; \</span>
<span class="go">&gt;    -DCMAKE_BUILD_TYPE=Release \</span>
<span class="go">&gt;    -DLLVM_ENABLE_ASSERTIONS=ON</span>

<span class="go">~/llvm/1/llvm-project/build$ cmake --build . --target check-mlir</span>
<span class="go">[200/1919] Generating VCSRevision.h</span>
<span class="go">-- Found Git: /usr/bin/git (found version &quot;2.17.1&quot;)</span>
<span class="go">[1604/1919] Building CXX object tools/mlir/tools/mlir-linalg-ods-gen/CMakeFiles/mlir-linalg-ods-gen.dir/mlir-linalg-ods-gen.cpp.o</span>
<span class="go">/home/cschen/llvm/1/llvm-project/mlir/tools/mlir-linalg-ods-gen/mlir-linalg-ods-gen.cpp:935:6: warning: ‘bool {anonymous}::Expression::operator==(const {anonymous}::Expression&amp;) const’ defined but not used [-Wunused-function]</span>
<span class="go"> bool Expression::operator==(const Expression &amp;e) const {</span>
<span class="go">      ^~~~~~~~~~</span>
<span class="go">[1918/1919] Running the MLIR regression tests</span>

<span class="go">Testing Time: 9.88s</span>
<span class="go">  Unsupported Tests:  16</span>
<span class="go">  Expected Passes  : 465</span>
</pre></div>
</div>
<p>run: <a class="reference external" href="https://mlir.llvm.org/docs/Tutorials/Toy/">https://mlir.llvm.org/docs/Tutorials/Toy/</a></p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">~/llvm/1/llvm-project/mlir/test/Examples/Toy/Ch1$ ~/llvm/1/llvm-project/build/bin/toyc-ch1 ast.toy -emit=ast</span>
<span class="go">...</span>
<span class="go">~/llvm/1/llvm-project/mlir/test/Examples/Toy/Ch1$ ~/llvm/1/llvm-project/build/bin/toyc-ch1 ast.toy -emit=ast 2&gt;&amp;1 | ~/llvm/1/llvm-project/build/bin/FileCheck ast.toy</span>
<span class="go">~/llvm/1/llvm-project/mlir/test/Examples/Toy/Ch1$ ~/llvm/1/llvm-project/build/bin/llvm-lit ast.toy</span>
<span class="go">-- Testing: 1 tests, 1 workers --</span>
<span class="go">PASS: MLIR :: Examples/Toy/Ch1/ast.toy (1 of 1)</span>

<span class="go">Testing Time: 0.11s</span>
<span class="go">  Expected Passes: 1</span>
</pre></div>
</div>
<p>The result I run is based on git commit 455ccde1377b3ec32d321eb7c38808fecdf230a8 Date:   Sun May 17 21:00:09 2020 -0400</p>
</section>
<section id="support-tensorflow">
<h3><a class="toc-backref" href="#id55" role="doc-backlink">Support tensorflow</a><a class="headerlink" href="#support-tensorflow" title="Permalink to this heading">¶</a></h3>
<p>Question:</p>
<p>Sean,</p>
<p>As I said, we can always redefine AI model to remove unknown type or dimension at ahead of time compilation to fit static compilation binding, and my AI input models are CNN without loop (it is DAG form). For this kind of models on tensorflow, can it be translated absolutely to mlir form based on what you know? If it can, then I can write converting program for mlir to my npu internal ir to support tensorflow.</p>
<p>Answer:</p>
<p>For programs with those restrictions, converting to MLIR xla_hlo dialect is always possible.</p>
<p>Note that it is always possible to convert a TensorFlow GraphDef into MLIR tensorflow dialect. MLIR is very flexible. But MLIR tensorflow dialect is too general for NPU and needs to be converted to MLIR xla_hlo dialect.</p>
<p>– Sean Silva</p>
<p>Sean,</p>
<p>Thank you! I am going to pass this information to my boss. We don’t study mlir yet. I believe it will take effort and we only have few engineers on compiler taking a lot of works. There other resource such as tensorflow-onnx but only part of supporting tensorflow to onnx converting.</p>
<p>Jonathan</p>
</section>
<section id="onnc">
<h3><a class="toc-backref" href="#id56" role="doc-backlink">ONNC</a><a class="headerlink" href="#onnc" title="Permalink to this heading">¶</a></h3>
<figure class="align-center" id="id36">
<span id="id23"></span><a class="reference internal image-reference" href="_images/onnc.jpg"><img alt="_images/onnc.jpg" src="_images/onnc.jpg" style="width: 504.0px; height: 378.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 82 </span><span class="caption-text">ONNC</span><a class="headerlink" href="#id36" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Private IR is better than llvm intrinsic IR for non-VLIW (SIMD or MIMD). Stride, memory dependences, …, llvm has not much advantages in this. Private IR is better.</p></li>
<li><p>Support MLIR to private IR in Novemember. Open source tensorflow to onnx has limited operations support only, bad and not good.</p></li>
<li><p>TVM support python interfaces but from TVM -&gt; relay is not good according their experience. From MLIR is better.</p></li>
<li><p>Support MLIR, pytorch, caffe are enough. Future has less AI model tools.</p></li>
<li><p>Run time scheduling customer support.</p></li>
<li><p>tf-onnx is not sufficient to support tf’s operators and bad. So, translate tf through MLIR to ONNC to customer IR is must.</p></li>
</ul>
<p><a class="reference external" href="https://onnc.ai/">https://onnc.ai/</a></p>
</section>
</section>
<section id="llvm-based-npu-compiler">
<h2><a class="toc-backref" href="#id57" role="doc-backlink">LLVM based NPU compiler</a><a class="headerlink" href="#llvm-based-npu-compiler" title="Permalink to this heading">¶</a></h2>
<p>Use LLVM rather than GCC because TVM open source compiler generating llvm-ir
for X86 and ARM and may extend to support other CPU in future.
Though TVM may uses BYOC to generate C function calling NPU’s builtin function,
the AutoTVM layer allows doing more optimization code generation such as
vectorization, CPU/NPU instructions iterleaving, …, etc <a class="footnote-reference brackets" href="#tvm-passes-stack" id="id24" role="doc-noteref"><span class="fn-bracket">[</span>26<span class="fn-bracket">]</span></a>
<a class="footnote-reference brackets" href="#best-tvm-ai-acce" id="id25" role="doc-noteref"><span class="fn-bracket">[</span>27<span class="fn-bracket">]</span></a>.
However in cloud of DL scenario, since the time of data transfer from global
DRAM to PE’s (Processor Entity) local memory SRAM is unknown until run time,
applying BYOC for calling NPU’s builtin function then using GCC instead of
llvm is possilbe.
NPU usually
implements data parallel instructions such as matrix multiplication,
convolution, relu, pool, …, to speed up the Deep Learning Operations.
For other operations not very data parallel such as global pool, concat, sort,
…, may leave to CPU finishing them. TVM output llvm-ir rather than GCC since
GCC community never had desire to enable any tools besides compiler (Richard
Stallman resisted attempts to make IR more reusable to prevent third-party
commercial tools from reusing GCC’s frontends) <a class="footnote-reference brackets" href="#llvm-ir-vs-gimple" id="id26" role="doc-noteref"><span class="fn-bracket">[</span>28<span class="fn-bracket">]</span></a>.</p>
<p>The way for supporting llvm based NPU compiler is to implement builtin functions
in clang and the corresponding specific NPU’s llvm-intrinsice functions in llvm
backend. For instance, the matrix multiplication operations of clang/llvm support
as the following table.</p>
<table class="docutils align-default" id="id37">
<caption><span class="caption-number">Table 50 </span><span class="caption-text">Matrix Multiplication defined in clang, llvm and ASM</span><a class="headerlink" href="#id37" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Function/IR/Instruction</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>clang’s builtin</p></td>
<td><p>__builtin_tensor_matmul(A, B, C)</p></td>
</tr>
<tr class="row-odd"><td><p>llvm’s intrinsic</p></td>
<td><p>&#64;llvm.lt.matmul %A, %B, %C</p></td>
</tr>
<tr class="row-even"><td><p>NPU’s ASM instruction</p></td>
<td><p>matmul $A, $B, %C</p></td>
</tr>
</tbody>
</table>
<p>The detail steps to support clang’s builtin and llvm’s intrinsic function for
backend are in my books <a class="footnote-reference brackets" href="#support-clang-builtin" id="id27" role="doc-noteref"><span class="fn-bracket">[</span>29<span class="fn-bracket">]</span></a>
<a class="footnote-reference brackets" href="#support-llvm-intrinsic-for-backend" id="id28" role="doc-noteref"><span class="fn-bracket">[</span>30<span class="fn-bracket">]</span></a>.</p>
<section id="llvm-ir-for-npu-compiler">
<h3><a class="toc-backref" href="#id58" role="doc-backlink">llvm IR for NPU compiler</a><a class="headerlink" href="#llvm-ir-for-npu-compiler" title="Permalink to this heading">¶</a></h3>
<p>Though npu has no general purpose registers GPR, it is possible to apply llvm ir for
npu to do codegen by llvm as follows,</p>
<figure class="align-center" id="id38">
<span id="conv"></span><a class="reference internal image-reference" href="_images/conv_onnx.png"><img alt="_images/conv_onnx.png" src="_images/conv_onnx.png" style="width: 286.0px; height: 201.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 83 </span><span class="caption-text">Conv operation in onnx file</span><a class="headerlink" href="#id38" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<div class="highlight-llvm notranslate"><div class="highlight"><pre><span></span><span class="vg">@x1</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="k">global</span><span class="w"> </span><span class="p">[</span><span class="m">1</span><span class="w"> </span><span class="k">x</span><span class="w"> </span><span class="p">[</span><span class="m">3</span><span class="w"> </span><span class="k">x</span><span class="w"> </span><span class="p">[</span><span class="m">120</span><span class="w"> </span><span class="k">x</span><span class="w"> </span><span class="p">[</span><span class="m">120</span><span class="w"> </span><span class="k">x</span><span class="w"> </span><span class="kt">float</span><span class="p">]]]],</span><span class="w"> </span><span class="k">align</span><span class="w"> </span><span class="m">4</span><span class="w"></span>
<span class="vg">@w1</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="k">global</span><span class="w"> </span><span class="p">[</span><span class="m">64</span><span class="w"> </span><span class="k">x</span><span class="w"> </span><span class="p">[</span><span class="m">3</span><span class="w"> </span><span class="k">x</span><span class="w"> </span><span class="p">[</span><span class="m">7</span><span class="w"> </span><span class="k">x</span><span class="w"> </span><span class="p">[</span><span class="m">7</span><span class="w"> </span><span class="k">x</span><span class="w"> </span><span class="kt">float</span><span class="p">]]]],</span><span class="w"> </span><span class="k">align</span><span class="w"> </span><span class="m">4</span><span class="w"></span>
<span class="vg">@conv</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="vg">@llvm.npu1.conv</span><span class="w"> </span><span class="kt">float</span><span class="p">*</span><span class="w"> </span><span class="vg">@x</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="p">*</span><span class="w"> </span><span class="vg">@weight</span><span class="p">,</span><span class="w"> </span><span class="p">...</span><span class="w"></span>
</pre></div>
</div>
<p>Conclusion:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>No GPRs in NPU but can get advantage of code-gen by llvm-tblgen tool.</p></li>
<li><p>The vector size of llvm is power of 2 (1, 2, 4, 8, …). But it can be achieved by modifying llvm kernel source data type.</p></li>
</ol>
<p>ref. code/llvm-ex1.c</p>
<ol class="arabic simple" start="3">
<li><p>Though NPU has no GPRs, the memory allocation can be done by adjust instructions order and split instructions (if over NPU’s memory) in passes of LLVM IR level.</p></li>
</ol>
</div></blockquote>
<p>reference:</p>
<blockquote>
<div><ul class="simple">
<li><p>section 5.2.2  Code Generation based on Low-Level IR.The low-level IR adopted by most DL compilers can be eventually lowered to LLVM IR, and benefits from LLVM’s mature optimizer and code generator <a class="footnote-reference brackets" href="#dlcs" id="id29" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a>.</p></li>
</ul>
</div></blockquote>
</section>
<section id="open-source-project">
<h3><a class="toc-backref" href="#id59" role="doc-backlink">Open source project</a><a class="headerlink" href="#open-source-project" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>onnx to mlir dialect: <a class="reference external" href="https://github.com/onnx/onnx-mlir">https://github.com/onnx/onnx-mlir</a></p></li>
<li><p>tensorflow to onnx: <a class="reference external" href="https://github.com/onnx/tensorflow-onnx">https://github.com/onnx/tensorflow-onnx</a></p></li>
<li><p>onnx to tensorflow: <a class="reference external" href="https://github.com/onnx/onnx-tensorflow">https://github.com/onnx/onnx-tensorflow</a></p></li>
</ul>
</section>
</section>
<section id="onnx">
<h2><a class="toc-backref" href="#id60" role="doc-backlink">ONNX</a><a class="headerlink" href="#onnx" title="Permalink to this heading">¶</a></h2>
<section id="viewer">
<h3><a class="toc-backref" href="#id61" role="doc-backlink">viewer</a><a class="headerlink" href="#viewer" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>Web for opening onnx <a class="reference external" href="https://lutzroeder.github.io/netron/">https://lutzroeder.github.io/netron/</a></p></li>
<li><p>Application tool for opening onnx  <a class="reference external" href="https://github.com/lutzroeder/netron">https://github.com/lutzroeder/netron</a>  // find “Browser: Start” in this page</p></li>
</ul>
<p>Netron app in ubuntu.</p>
</section>
<section id="install">
<h3><a class="toc-backref" href="#id62" role="doc-backlink">install</a><a class="headerlink" href="#install" title="Permalink to this heading">¶</a></h3>
<p>$ pip install onnx</p>
</section>
<section id="onnx-api">
<h3><a class="toc-backref" href="#id63" role="doc-backlink">onnx api</a><a class="headerlink" href="#onnx-api" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>copy onnx file: ex. nc/code/copy_onnx.py referenece here <a class="footnote-reference brackets" href="#copy1" id="id30" role="doc-noteref"><span class="fn-bracket">[</span>23<span class="fn-bracket">]</span></a>.</p></li>
<li><p>create onnx file: ex. nc/code/create1.py referenece here <a class="footnote-reference brackets" href="#create" id="id31" role="doc-noteref"><span class="fn-bracket">[</span>24<span class="fn-bracket">]</span></a>.</p></li>
<li><p>Kneron onnx creating tool <a class="footnote-reference brackets" href="#kneron-onnx-create-tool" id="id32" role="doc-noteref"><span class="fn-bracket">[</span>25<span class="fn-bracket">]</span></a>.</p></li>
</ul>
<aside class="footnote brackets" id="hungyilee" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://www.youtube.com/watch?v=CXgbekl66jc&amp;list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49">https://www.youtube.com/watch?v=CXgbekl66jc&amp;list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49</a></p>
</aside>
<aside class="footnote brackets" id="selectedpattern" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">2</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="http://violin-tao.blogspot.com/2017/07/ml-convolutional-neural-network-cnn.html">http://violin-tao.blogspot.com/2017/07/ml-convolutional-neural-network-cnn.html</a></p>
</aside>
<aside class="footnote brackets" id="onnxmodelzoo" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">3</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://github.com/onnx/models">https://github.com/onnx/models</a></p>
</aside>
<aside class="footnote brackets" id="cnnwiki" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">4</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Convolutional_neural_network">https://en.wikipedia.org/wiki/Convolutional_neural_network</a></p>
</aside>
<aside class="footnote brackets" id="gru" role="note">
<span class="label"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://medium.com/&#64;tengyuanchang/%E6%AF%94%E8%BC%83%E9%95%B7%E7%9F%AD%E6%9C%9F%E8%A8%98%E6%86%B6%E6%A8%A1%E5%9E%8B-lstm-%E8%88%87%E6%94%B9%E8%89%AF%E5%BE%8C%E7%9A%84%E9%81%9E%E6%AD%B8%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E6%A8%A1%E5%9E%8B-gru-813dec22ec6d">https://medium.com/&#64;tengyuanchang/%E6%AF%94%E8%BC%83%E9%95%B7%E7%9F%AD%E6%9C%9F%E8%A8%98%E6%86%B6%E6%A8%A1%E5%9E%8B-lstm-%E8%88%87%E6%94%B9%E8%89%AF%E5%BE%8C%E7%9A%84%E9%81%9E%E6%AD%B8%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E6%A8%A1%E5%9E%8B-gru-813dec22ec6d</a></p>
</aside>
<aside class="footnote brackets" id="gru2" role="note">
<span class="label"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></span>
<p>in video 22:55 <a class="reference external" href="https://www.youtube.com/watch?v=rTqmWlnwz_0&amp;list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&amp;index=31">https://www.youtube.com/watch?v=rTqmWlnwz_0&amp;list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&amp;index=31</a></p>
</aside>
<aside class="footnote brackets" id="dlcs" role="note">
<span class="label"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id5">1</a>,<a role="doc-backlink" href="#id29">2</a>)</span>
<p><a class="reference external" href="https://arxiv.org/pdf/2002.03794.pdf">https://arxiv.org/pdf/2002.03794.pdf</a></p>
</aside>
<aside class="footnote brackets" id="gpu4dl" role="note">
<span class="label"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id6">1</a>,<a role="doc-backlink" href="#id7">2</a>)</span>
<p><a class="reference external" href="https://missinglink.ai/guides/tensorflow/tensorflow-support-opencl/">https://missinglink.ai/guides/tensorflow/tensorflow-support-opencl/</a></p>
</aside>
<aside class="footnote brackets" id="id33" role="note">
<span class="label"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id8">1</a>,<a role="doc-backlink" href="#id9">2</a>,<a role="doc-backlink" href="#id10">3</a>)</span>
<p><a class="reference external" href="https://www.khronos.org/nnef">https://www.khronos.org/nnef</a></p>
</aside>
<aside class="footnote brackets" id="id34" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">10</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://www.khronos.org/openvx">https://www.khronos.org/openvx</a></p>
</aside>
<aside class="footnote brackets" id="openvx-wiki" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">11</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/OpenVX">https://en.wikipedia.org/wiki/OpenVX</a></p>
</aside>
<aside class="footnote brackets" id="id35" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">12</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://www.khronos.org/sycl">https://www.khronos.org/sycl</a></p>
</aside>
<aside class="footnote brackets" id="sycl-wiki" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">13</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/SYCL">https://en.wikipedia.org/wiki/SYCL</a></p>
</aside>
<aside class="footnote brackets" id="onnx-editor" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">14</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://www.youtube.com/watch?v=QZQwmZTfLmI">https://www.youtube.com/watch?v=QZQwmZTfLmI</a></p>
</aside>
<aside class="footnote brackets" id="iree" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">15</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://github.com/google/iree">https://github.com/google/iree</a></p>
</aside>
<aside class="footnote brackets" id="tfunknownshape" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">16</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://pgaleone.eu/tensorflow/2018/07/28/understanding-tensorflow-tensors-shape-static-dynamic/">https://pgaleone.eu/tensorflow/2018/07/28/understanding-tensorflow-tensors-shape-static-dynamic/</a></p>
</aside>
<aside class="footnote brackets" id="vm-ir-dml" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id18">17</a><span class="fn-bracket">]</span></span>
<p>Page 15 of <a class="reference external" href="https://docs.google.com/presentation/d/1RCQ4ZPQFK9cVgu3IH1e5xbrBcqy7d_cEZ578j84OvYI/edit#slide=id.g6e31674683_0_23101">https://docs.google.com/presentation/d/1RCQ4ZPQFK9cVgu3IH1e5xbrBcqy7d_cEZ578j84OvYI/edit#slide=id.g6e31674683_0_23101</a></p>
</aside>
<aside class="footnote brackets" id="mlir-iree-purpose" role="note">
<span class="label"><span class="fn-bracket">[</span>18<span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://kknews.cc/zh-tw/tech/klkombr.html">https://kknews.cc/zh-tw/tech/klkombr.html</a></p>
</aside>
<aside class="footnote brackets" id="onnx-fmt" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id19">19</a><span class="fn-bracket">]</span></span>
<p>Actually onnx format based on IO api with protobuffer. It has real binary format but may change from version to version. Tensorflow api has no real binary format.</p>
</aside>
<aside class="footnote brackets" id="onnx-mlir" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">20</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://github.com/onnx/onnx-mlir">https://github.com/onnx/onnx-mlir</a></p>
</aside>
<aside class="footnote brackets" id="onnx-mlir-sponsor" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id21">21</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://groups.google.com/a/tensorflow.org/forum/#!topic/mlir/2FT4sD8kqTY">https://groups.google.com/a/tensorflow.org/forum/#!topic/mlir/2FT4sD8kqTY</a></p>
</aside>
<aside class="footnote brackets" id="tf-onnx" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id22">22</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://github.com/onnx/tensorflow-onnx">https://github.com/onnx/tensorflow-onnx</a></p>
</aside>
<aside class="footnote brackets" id="copy1" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id30">23</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://github.com/onnx/onnx/issues/2052">https://github.com/onnx/onnx/issues/2052</a></p>
</aside>
<aside class="footnote brackets" id="create" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id31">24</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://www.google.com/search?client=ubuntu&amp;hs=bS9&amp;channel=fs&amp;sxsrf=ALeKk00IITG3Dj_IryeytZ_iTJJE3PszMA%3A1597217944046&amp;ei=mJwzX8KZAuiLr7wPr_aq0AU&amp;q=onnx+python+api&amp;oq=onnx+python+api&amp;gs_lcp=CgZwc3ktYWIQAzIECCMQJzIGCAAQCBAeOggIABAHEB4QEzoKCAAQCBAHEB4QE1CAEFiAEGCNHmgAcAB4AIABdIgBjwKSAQMxLjKYAQCgAQGqAQdnd3Mtd2l6wAEB&amp;sclient=psy-ab&amp;ved=0ahUKEwjCxbvBlJXrAhXoxYsBHS-7CloQ4dUDCAs&amp;uact=5">https://www.google.com/search?client=ubuntu&amp;hs=bS9&amp;channel=fs&amp;sxsrf=ALeKk00IITG3Dj_IryeytZ_iTJJE3PszMA%3A1597217944046&amp;ei=mJwzX8KZAuiLr7wPr_aq0AU&amp;q=onnx+python+api&amp;oq=onnx+python+api&amp;gs_lcp=CgZwc3ktYWIQAzIECCMQJzIGCAAQCBAeOggIABAHEB4QEzoKCAAQCBAHEB4QE1CAEFiAEGCNHmgAcAB4AIABdIgBjwKSAQMxLjKYAQCgAQGqAQdnd3Mtd2l6wAEB&amp;sclient=psy-ab&amp;ved=0ahUKEwjCxbvBlJXrAhXoxYsBHS-7CloQ4dUDCAs&amp;uact=5</a></p>
</aside>
<aside class="footnote brackets" id="kneron-onnx-create-tool" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id32">25</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://github.com/kneron/ONNX_Convertor/blob/master/optimizer_scripts/onnx2onnx.py">https://github.com/kneron/ONNX_Convertor/blob/master/optimizer_scripts/onnx2onnx.py</a></p>
</aside>
<aside class="footnote brackets" id="tvm-passes-stack" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id24">26</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://discuss.tvm.apache.org/t/how-to-see-different-ir-relay-te-tir-from-my-own-pytorch-model/13684">https://discuss.tvm.apache.org/t/how-to-see-different-ir-relay-te-tir-from-my-own-pytorch-model/13684</a></p>
</aside>
<aside class="footnote brackets" id="best-tvm-ai-acce" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id25">27</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://discuss.tvm.apache.org/t/which-is-the-best-way-to-port-tvm-to-a-new-ai-accelerator/6905">https://discuss.tvm.apache.org/t/which-is-the-best-way-to-port-tvm-to-a-new-ai-accelerator/6905</a></p>
</aside>
<aside class="footnote brackets" id="llvm-ir-vs-gimple" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id26">28</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://stackoverflow.com/questions/40799696/how-is-gcc-ir-different-from-llvm-ir/40802063">https://stackoverflow.com/questions/40799696/how-is-gcc-ir-different-from-llvm-ir/40802063</a></p>
</aside>
<aside class="footnote brackets" id="support-clang-builtin" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id27">29</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="http://jonathan2251.github.io/lbt/clang.html#builtin-functions">http://jonathan2251.github.io/lbt/clang.html#builtin-functions</a></p>
</aside>
<aside class="footnote brackets" id="support-llvm-intrinsic-for-backend" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id28">30</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="http://jonathan2251.github.io/lbd/funccall.html#add-specific-backend-intrinsic-function">http://jonathan2251.github.io/lbd/funccall.html#add-specific-backend-intrinsic-function</a></p>
</aside>
</section>
</section>
</section>


      </div>
      <div class="bottomnav" role="navigation" aria-label="bottom navigation">
      
        <p>
        «&#160;&#160;<a href="doc.html">Appendix B: Cpu0 document and test</a>
        &#160;&#160;::&#160;&#160;
        <a class="uplink" href="index.html">Contents</a>
        &#160;&#160;::&#160;&#160;
        <a href="todo.html">Todo List</a>&#160;&#160;»
        </p>

      </div>

    <div class="footer" role="contentinfo">
        &#169; Copyright 2016, Chen Chung-Shu.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.0.2.
    </div>
  </body>
</html>